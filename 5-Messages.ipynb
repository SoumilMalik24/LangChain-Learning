{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6638bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = init_chat_model(model=\"gpt-5\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39899b4e",
   "metadata": {},
   "source": [
    "## Text Prompts\n",
    "\n",
    "String - ideal for straight forward text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c1ca6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here’s a concise overview of AI:\\n\\n- What it is: Artificial intelligence builds systems that perform tasks that typically require human intelligence—perception, language understanding, decision-making, and learning—by finding patterns in data rather than following only hand-coded rules.\\n\\n- Main approaches:\\n  - Machine learning: models learn from examples; includes classical methods (linear models, trees) and deep learning.\\n  - Deep learning: neural networks with many layers; powers modern vision, speech, and language systems.\\n  - Generative models: create text, images, audio, or code (e.g., large language models and diffusion models).\\n  - Reinforcement learning: agents learn by trial and error with rewards.\\n  - Symbolic/knowledge-based methods: logic and rules; sometimes combined with learning (neuro-symbolic).\\n\\n- How modern LLMs work (briefly): Trained on large text datasets to predict the next token. They learn statistical relationships (syntax, facts, reasoning patterns). At inference, they generate one token at a time, optionally using tools or retrieved documents (RAG) to ground answers.\\n\\n- What AI is good at: pattern recognition at scale, summarization, translation, recommendations, anomaly detection, code assistance, creative drafting, and perception (vision/audio).\\n\\n- Limitations and risks: can be wrong or overconfident (“hallucinations”), biased by training data, data-hungry, sometimes brittle out of distribution, privacy/security concerns, IP/use-rights issues, and energy/compute costs. Human oversight and evaluation are important.\\n\\n- Applications: healthcare triage and imaging, drug discovery, fraud detection, supply-chain forecasting, autonomous systems and robotics, customer support, education/tutoring, accessibility tools, scientific simulation and design.\\n\\n- Trends: multimodal models (text–image–audio–video), agentic systems that use tools and browse, retrieval-augmented generation for factuality, smaller efficient on-device models, privacy-preserving techniques, and growing regulation (e.g., EU AI Act, NIST AI Risk Management Framework).\\n\\nIf you’d like, tell me which aspect you’re most interested in (e.g., how LLMs work, practical use cases, risks, or how to get started), and I’ll go deeper.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1171, 'prompt_tokens': 12, 'total_tokens': 1183, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CzKIcEjIJqd6VTfFSbBcy1RuwM6Up', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bd0a5-9a7a-70f1-a748-dda56b5e5e45-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 1171, 'total_tokens': 1183, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Please tell me something about AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9717c0",
   "metadata": {},
   "source": [
    "## Message Prompts\n",
    "#### List of messages\n",
    "\n",
    "Message Types:-\n",
    "- System Message\n",
    "- Human Message\n",
    "- AI Message\n",
    "- Tool Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbce82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) is a pattern that pairs a search/retrieval system with a generative language model so the model answers using relevant, up-to-date external knowledge rather than only its frozen parameters. In practice, a RAG application lets users ask questions in natural language and get grounded answers with citations to your documents, databases, or APIs.\n",
      "\n",
      "Core architecture\n",
      "- Data ingestion and indexing\n",
      "  - Collect sources (docs, webpages, PDFs, tickets, code, DB rows).\n",
      "  - Chunk/split text (typical: 200–500 tokens with 10–20% overlap) and attach metadata (source, timestamp, tags).\n",
      "  - Create embeddings and build an index (vector DB). Optionally also index for sparse/BM25.\n",
      "- Retrieval\n",
      "  - Convert the user query to an embedding and retrieve top-k chunks.\n",
      "  - Often use hybrid retrieval (dense vectors + BM25) and filters (metadata, permissions).\n",
      "  - Optional reranking with a cross-encoder to pick the most relevant 3–10 passages.\n",
      "- Prompting and generation\n",
      "  - Construct a prompt that includes the question + selected passages + instructions to cite sources and refuse when uncertain.\n",
      "  - Call the LLM to synthesize an answer grounded in those passages.\n",
      "- Post-processing and UX\n",
      "  - Return answer, citations, highlighted quotes; stream tokens for responsiveness.\n",
      "  - Caching, result deduplication, and safety/PII filters.\n",
      "- Evaluation and monitoring\n",
      "  - Track retrieval quality, groundedness/faithfulness, answer relevance, latency, and user feedback.\n",
      "\n",
      "Why use RAG\n",
      "- Freshness and control: inject new or private data instantly without model retraining.\n",
      "- Traceability: show sources for auditability/compliance.\n",
      "- Cost/latency: cheaper and faster than fine-tuning for most knowledge tasks.\n",
      "- Security: keep data in your stack; apply row-/document-level access controls at retrieval time.\n",
      "\n",
      "Limitations and pitfalls\n",
      "- Retrieval is the bottleneck: if relevant passages aren’t retrieved, the LLM can’t answer well.\n",
      "- Hallucinations can still occur; instruct the model to only use provided sources and to say “I don’t know.”\n",
      "- Chunking mistakes: chunks too big dilute relevance; too small lose context.\n",
      "- Long-context models reduce but don’t eliminate the need for retrieval and can be slower/costlier.\n",
      "- Multi-hop questions need query decomposition or iterative retrieval.\n",
      "- Latency from multiple hops (embedding, vector search, rerank, LLM) requires careful optimization.\n",
      "\n",
      "Best practices and key design choices\n",
      "- Chunking: start with 300–400 tokens, 15% overlap; use semantic or header-aware splitters for structured docs.\n",
      "- Embeddings: choose strong, domain-suitable models; consider multilingual if needed.\n",
      "- Retrieval: use hybrid search; start with k=20–50, rerank to top 3–7 with a cross-encoder; apply metadata filters.\n",
      "- Prompting: include concise instructions, the user query, the retrieved context, and a format for citations. Keep within context limits.\n",
      "- Hallucination mitigation: require quotes/snippets, confidence statements, and allow “not found.” Optionally add a verifier step that checks claims against sources.\n",
      "- Access control: filter by tenant/user permissions in the retriever, not in the prompt.\n",
      "- Performance: cache embeddings and retrievals; reuse conversations’ short-term memory; stream outputs; parallelize retrieval and rerank.\n",
      "- Evaluation: build a test set (gold Q/A with sources). Use automated RAG evals (faithfulness, answer correctness, context precision/recall) and human review for critical flows.\n",
      "- Operations: version your index, handle updates/invalidation, de-duplicate, monitor drift, and log prompts/retrievals for debugging.\n",
      "\n",
      "Advanced patterns\n",
      "- Query rewriting/expansion (e.g., HyDE) to improve recall; language normalization; spelling correction.\n",
      "- Multi-step or multi-hop retrieval (decompose question -> retrieve -> subanswers -> compose).\n",
      "- Reranking with strong cross-encoders; late fusion of results from multiple indices.\n",
      "- Knowledge graph or SQL tool use alongside RAG for structured queries.\n",
      "- Agentic RAG: the model plans retrieval steps and tool calls when needed.\n",
      "\n",
      "When to use vs alternatives\n",
      "- Use RAG for retrieval-style QA, customer support, policy and compliance search, research assistants, code and API assistants, and enterprise search chat.\n",
      "- Prefer fine-tuning when you need new skills or style that retrieval can’t provide (e.g., domain-specific reasoning patterns).\n",
      "- Prefer classic search/BI when exactness/traceability over structured data is required; RAG can call SQL instead of summarizing raw tables.\n",
      "\n",
      "Minimal implementation recipe\n",
      "- Ingest: load documents, clean, split into chunks, add metadata.\n",
      "- Index: embed chunks and load into a vector store; optionally build a BM25 index.\n",
      "- Serve:\n",
      "  - Embed user query.\n",
      "  - Retrieve top-k; apply filters; rerank to top-n.\n",
      "  - Build prompt with instructions + context + question.\n",
      "  - Generate with LLM; include citations (source IDs/URLs).\n",
      "  - Return answer; log query, retrieved contexts, and feedback.\n",
      "- Evaluate and iterate: tune chunk size, k/n, hybrid weights, reranker, and prompt; add guardrails.\n",
      "\n",
      "Tooling ecosystem (examples, not endorsements)\n",
      "- Frameworks: LangChain, LlamaIndex, Haystack, Semantic Kernel.\n",
      "- Vector stores/search: Pinecone, Weaviate, Milvus, FAISS, pgvector, Elasticsearch/OpenSearch (hybrid).\n",
      "- Embeddings and rerankers: OpenAI text-embedding-3, Cohere Embed/Rerank, Voyage, bge/e5 families, cross-encoder/ms-marco.\n",
      "- Observability/eval: RAGAS, DeepEval, TruLens, Langfuse, Arize Phoenix.\n",
      "- Guardrails: NeMo Guardrails, Guardrails AI, policy filters.\n",
      "\n",
      "If you share your data domain, latency/cost constraints, and deployment stack, I can suggest a concrete design and parameter choices.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "message=[\n",
    "    SystemMessage(\"You are a data science expert\"),\n",
    "    HumanMessage(\"Explain about RAG Application\")\n",
    "]\n",
    "\n",
    "response = model.invoke(message)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f455eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: you usually don’t “build one from scratch.” You stand up a vector store and load it with embeddings you generate from text/images/audio. Below are two practical ways to create a vector database you can use today.\n",
      "\n",
      "Option A: Qdrant (purpose-built vector DB, easy to run)\n",
      "1) Run Qdrant\n",
      "- With Docker:\n",
      "  docker run -p 6333:6333 -v qdrant_storage:/qdrant/storage qdrant/qdrant\n",
      "\n",
      "2) Install client and an embedding model\n",
      "- pip install qdrant-client sentence-transformers\n",
      "\n",
      "3) Embed, create a collection, upsert, and search (Python)\n",
      "- Example uses the 384-dim all-MiniLM-L6-v2 model.\n",
      "\n",
      "from sentence_transformers import SentenceTransformer\n",
      "from qdrant_client import QdrantClient\n",
      "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
      "\n",
      "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "client = QdrantClient(host=\"localhost\", port=6333)\n",
      "\n",
      "# Create or reset a collection\n",
      "client.recreate_collection(\n",
      "    collection_name=\"docs\",\n",
      "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
      ")\n",
      "\n",
      "texts = [\n",
      "    {\"id\": 1, \"text\": \"Paris is the capital of France.\", \"source\": \"wiki\"},\n",
      "    {\"id\": 2, \"text\": \"Berlin is known for its art scene.\", \"source\": \"wiki\"},\n",
      "    {\"id\": 3, \"text\": \"The Eiffel Tower is in Paris.\", \"source\": \"guide\"},\n",
      "]\n",
      "vectors = model.encode([t[\"text\"] for t in texts]).tolist()\n",
      "\n",
      "points = [\n",
      "    PointStruct(id=t[\"id\"], vector=v, payload=t) for t, v in zip(texts, vectors)\n",
      "]\n",
      "client.upsert(collection_name=\"docs\", points=points)\n",
      "\n",
      "# Query\n",
      "query = \"famous landmark in the capital of France\"\n",
      "qvec = model.encode([query])[0].tolist()\n",
      "hits = client.search(collection_name=\"docs\", query_vector=qvec, limit=3)\n",
      "\n",
      "for hit in hits:\n",
      "    print(hit.id, hit.score, hit.payload[\"text\"])\n",
      "\n",
      "- Add metadata filtering:\n",
      "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
      "flt = Filter(must=[FieldCondition(key=\"source\", match=MatchValue(value=\"wiki\"))])\n",
      "hits = client.search(\"docs\", qvec, limit=3, query_filter=flt)\n",
      "\n",
      "Option B: PostgreSQL + pgvector (great if you already use Postgres)\n",
      "1) Install pgvector and Postgres\n",
      "- macOS (Homebrew): brew install postgresql\n",
      "- In psql: CREATE EXTENSION IF NOT EXISTS vector;\n",
      "\n",
      "2) Create a table and index\n",
      "CREATE TABLE docs (\n",
      "  id BIGSERIAL PRIMARY KEY,\n",
      "  text TEXT,\n",
      "  metadata JSONB,\n",
      "  embedding vector(384)\n",
      ");\n",
      "CREATE INDEX ON docs USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n",
      "ANALYZE docs;\n",
      "\n",
      "3) Generate embeddings and insert (Python)\n",
      "from sentence_transformers import SentenceTransformer\n",
      "import psycopg2, json\n",
      "\n",
      "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "conn = psycopg2.connect(\"dbname=postgres user=postgres password=postgres host=localhost\")\n",
      "cur = conn.cursor()\n",
      "\n",
      "texts = [\n",
      "    (\"Paris is the capital of France.\", {\"source\":\"wiki\"}),\n",
      "    (\"Berlin is known for its art scene.\", {\"source\":\"wiki\"}),\n",
      "    (\"The Eiffel Tower is in Paris.\", {\"source\":\"guide\"}),\n",
      "]\n",
      "vecs = model.encode([t[0] for t in texts]).tolist()\n",
      "\n",
      "for (t, m), v in zip(texts, vecs):\n",
      "    cur.execute(\"INSERT INTO docs (text, metadata, embedding) VALUES (%s, %s, %s)\", (t, json.dumps(m), v))\n",
      "conn.commit()\n",
      "\n",
      "4) Query by similarity\n",
      "q = \"famous landmark in the capital of France\"\n",
      "qv = model.encode([q])[0].tolist()\n",
      "cur.execute(\"\"\"\n",
      "  SELECT id, text, 1 - (embedding <=> %s) AS cosine_sim\n",
      "  FROM docs\n",
      "  ORDER BY embedding <=> %s\n",
      "  LIMIT 3\n",
      "\"\"\", (qv, qv))\n",
      "print(cur.fetchall())\n",
      "\n",
      "Key decisions and tips\n",
      "- Embeddings: pick a model that fits your data size and language (e.g., sentence-transformers locally, or a hosted API).\n",
      "- Distance metric: cosine is common for text embeddings.\n",
      "- Chunking: split long documents (e.g., 200–400 tokens) and store chunk text + metadata with each vector.\n",
      "- Filters: store metadata (source, id, tags) to support filtered search.\n",
      "- Indexing: Qdrant defaults to HNSW; in Postgres use ivfflat with ANALYZE. Tune index parameters for recall/latency.\n",
      "- Persistence and scale: Qdrant, Weaviate, Milvus, or managed services (e.g., Pinecone) simplify scaling and replicas. Postgres + pgvector is great when you want one database for everything.\n",
      "- Security/backups: secure your DB, back up storage, and consider access control.\n",
      "\n",
      "If you share your stack (language), data size, and whether you prefer managed or self-hosted, I can tailor a setup and code samples to your needs.\n"
     ]
    }
   ],
   "source": [
    "system_msg = SystemMessage(content=\"You are a helpful coding assistant\")\n",
    "\n",
    "messages = [\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do i create a vector database?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda3320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Below is a pragmatic path to building a REST API in Python, with concrete examples. I’ll show FastAPI (modern, fast, built-in validation/docs), then tiny Flask and Django REST Framework snippets so you can choose what fits.\\n\\nPick a framework\\n- FastAPI: Best developer experience, async, data validation with Pydantic, automatic OpenAPI docs at /docs.\\n- Flask: Minimal and flexible, you add what you need.\\n- Django REST Framework (DRF): Full-stack with ORM, auth, admin—great for bigger projects.\\n\\nFastAPI: quick, production-ready example\\n1) Setup\\n- Python 3.10+ recommended\\n- Create a virtual env and install:\\n  pip install fastapi uvicorn\\n\\n2) Minimal project structure (you can start with a single file)\\n- app.py\\n\\n3) Code (CRUD for a resource with validation, pagination, search, proper status codes)\\n# app.py\\nfrom uuid import UUID, uuid4\\nfrom typing import Optional\\n\\nfrom fastapi import FastAPI, HTTPException, status, Depends\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\\nfrom pydantic import BaseModel, Field\\n\\n# Pydantic models\\nclass ItemIn(BaseModel):\\n    name: str = Field(..., min_length=1, max_length=100)\\n    price: float = Field(..., gt=0)\\n    tags: list[str] = []\\n\\nclass Item(ItemIn):\\n    id: UUID\\n\\n# Fake in-memory store (swap with DB later)\\ndb: dict[UUID, Item] = {}\\n\\napp = FastAPI(title=\"Store API\", version=\"1.0.0\")\\n\\n# CORS (configure origins per your UI)\\napp.add_middleware(\\n    CORSMiddleware,\\n    allow_origins=[\"*\"],\\n    allow_methods=[\"*\"],\\n    allow_headers=[\"*\"],\\n)\\n\\n# Simple bearer auth example (replace with real JWT)\\nsecurity = HTTPBearer()\\n\\ndef get_user(creds: HTTPAuthorizationCredentials = Depends(security)):\\n    token = creds.credentials\\n    if token != \"secret-token\":\\n        raise HTTPException(status_code=401, detail=\"Invalid or missing token\")\\n    return {\"user\": \"demo\"}\\n\\n# Endpoints\\n@app.get(\"/v1/items\", response_model=list[Item])\\ndef list_items(q: Optional[str] = None, limit: int = 50, offset: int = 0):\\n    items = list(db.values())\\n    if q:\\n        items = [i for i in items if q.lower() in i.name.lower()]\\n    return items[offset: offset + limit]\\n\\n@app.post(\"/v1/items\", response_model=Item, status_code=status.HTTP_201_CREATED, dependencies=[Depends(get_user)])\\ndef create_item(item: ItemIn):\\n    new_item = Item(id=uuid4(), **item.model_dump())\\n    db[new_item.id] = new_item\\n    return new_item\\n\\n@app.get(\"/v1/items/{item_id}\", response_model=Item)\\ndef get_item(item_id: UUID):\\n    item = db.get(item_id)\\n    if not item:\\n        raise HTTPException(status_code=404, detail=\"Item not found\")\\n    return item\\n\\n@app.put(\"/v1/items/{item_id}\", response_model=Item, dependencies=[Depends(get_user)])\\ndef update_item(item_id: UUID, item: ItemIn):\\n    if item_id not in db:\\n        raise HTTPException(status_code=404, detail=\"Item not found\")\\n    updated = Item(id=item_id, **item.model_dump())\\n    db[item_id] = updated\\n    return updated\\n\\n@app.delete(\"/v1/items/{item_id}\", status_code=status.HTTP_204_NO_CONTENT, dependencies=[Depends(get_user)])\\ndef delete_item(item_id: UUID):\\n    if item_id not in db:\\n        raise HTTPException(status_code=404, detail=\"Item not found\")\\n    del db[item_id]\\n    return None\\n\\n4) Run and test\\n- Run: uvicorn app:app --reload\\n- Docs: http://127.0.0.1:8000/docs\\n- Test with curl:\\n  curl -s http://127.0.0.1:8000/v1/items\\n  curl -s -X POST http://127.0.0.1:8000/v1/items \\\\\\n    -H \"Content-Type: application/json\" \\\\\\n    -H \"Authorization: Bearer secret-token\" \\\\\\n    -d \\'{\"name\":\"Pen\",\"price\":1.5,\"tags\":[\"office\"]}\\'\\n\\n5) Add a database (SQLite + SQLAlchemy, very brief example)\\n- Install: pip install sqlalchemy aiosqlite\\n- Sketch:\\n  - Define SQLAlchemy models, create a SessionLocal (or async session), and replace the in-memory db with real queries in your endpoints.\\n  - Use alembic for migrations.\\n\\nFlask: minimal example\\n- Install: pip install flask\\n- app.py:\\nfrom flask import Flask, request, jsonify, abort\\nfrom uuid import uuid4\\n\\napp = Flask(__name__)\\ndb = {}\\n\\n@app.get(\"/items\")\\ndef list_items():\\n    return jsonify(list(db.values()))\\n\\n@app.post(\"/items\")\\ndef create_item():\\n    data = request.get_json() or {}\\n    if \"name\" not in data or \"price\" not in data:\\n        abort(400, \"name and price are required\")\\n    item_id = str(uuid4())\\n    item = {\"id\": item_id, \"name\": data[\"name\"], \"price\": data[\"price\"], \"tags\": data.get(\"tags\", [])}\\n    db[item_id] = item\\n    return item, 201\\n\\n@app.get(\"/items/<item_id>\")\\ndef get_item(item_id):\\n    item = db.get(item_id)\\n    if not item:\\n        abort(404)\\n    return item\\n\\n@app.put(\"/items/<item_id>\")\\ndef update_item(item_id):\\n    if item_id not in db:\\n        abort(404)\\n    data = request.get_json() or {}\\n    db[item_id].update({k: v for k, v in data.items() if k in {\"name\", \"price\", \"tags\"}})\\n    return db[item_id]\\n\\n@app.delete(\"/items/<item_id>\")\\ndef delete_item(item_id):\\n    if item_id not in db:\\n        abort(404)\\n    del db[item_id]\\n    return \"\", 204\\n\\n- Run: flask --app app run --debug\\n\\nDjango REST Framework: model-driven example\\n1) Install and start project\\n- pip install django djangorestframework\\n- django-admin startproject api_proj\\n- cd api_proj\\n- python manage.py startapp items\\n- Add \"rest_framework\" and \"items\" to INSTALLED_APPS\\n\\n2) items/models.py\\nfrom django.db import models\\n\\nclass Item(models.Model):\\n    name = models.CharField(max_length=100)\\n    price = models.DecimalField(max_digits=10, decimal_places=2)\\n    tags = models.JSONField(default=list)\\n\\n3) items/serializers.py\\nfrom rest_framework import serializers\\nfrom .models import Item\\n\\nclass ItemSerializer(serializers.ModelSerializer):\\n    class Meta:\\n        model = Item\\n        fields = \"__all__\"\\n\\n4) items/views.py\\nfrom rest_framework.viewsets import ModelViewSet\\nfrom .models import Item\\nfrom .serializers import ItemSerializer\\n\\nclass ItemViewSet(ModelViewSet):\\n    queryset = Item.objects.all()\\n    serializer_class = ItemSerializer\\n\\n5) api_proj/urls.py\\nfrom django.contrib import admin\\nfrom django.urls import path, include\\nfrom rest_framework.routers import DefaultRouter\\nfrom items.views import ItemViewSet\\n\\nrouter = DefaultRouter()\\nrouter.register(r\"items\", ItemViewSet)\\n\\nurlpatterns = [\\n    path(\"admin/\", admin.site.urls),\\n    path(\"api/\", include(router.urls)),\\n]\\n\\n6) Migrate and run\\n- python manage.py migrate\\n- python manage.py runserver\\n- Browse: http://127.0.0.1:8000/api/items/\\n\\nKey REST best practices\\n- Resource-oriented URIs: /v1/items, /v1/items/{id}\\n- Use proper HTTP methods and status codes\\n- Validate input, return errors consistently with JSON bodies\\n- Pagination: limit, offset or page, page_size\\n- Filtering and sorting via query params\\n- Versioning: prefix routes with /v1\\n- Auth: Bearer JWT for stateless APIs; use HTTPS only\\n- CORS if serving browsers\\n- Automated docs: FastAPI provides OpenAPI at /docs; in Flask use flask-smorest or drf-yasg for DRF\\n- Logging and structured error handling\\n- Tests: use pytest + httpx (FastAPI), pytest-flask, or Django’s test client\\n\\nBasic deployment\\n- FastAPI:\\n  - Production server example:\\n    pip install gunicorn uvicorn\\n    gunicorn -k uvicorn.workers.UvicornWorker app:app -w 2 -b 0.0.0.0:8000\\n  - Put Nginx in front for TLS and buffering\\n  - Containerize with Docker if needed\\n- DRF: run with gunicorn + Nginx; use whitenoise or object storage for static files\\n\\nIf you share your constraints (use case, auth needs, database choice, deployment target), I can tailor a starter template with the right stack and project structure.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 3150, 'prompt_tokens': 48, 'total_tokens': 3198, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CzKLkmiEiATy7q1QQpXQ3PCZTDo1o', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bd0a8-9366-7c73-bd37-94147309b0e6-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 48, 'output_tokens': 3150, 'total_tokens': 3198, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}\n"
     ]
    }
   ],
   "source": [
    "### Detailed info to LLM with System Message\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "\n",
    "system_msg = SystemMessage(\"\"\"\n",
    "You are a senior Python Developer with expertise in web framework.\n",
    "always provide code examples and explain your reasoning\n",
    "Be concise but throughout in your explanation\n",
    "\"\"\")\n",
    "\n",
    "messages = [\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do i create a REST API?\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b6813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message Metadata\n",
    "human_msg = HumanMessage(\n",
    "    content=\"Hello!\",\n",
    "    name=\"Soumil\",\n",
    "    id = \"msg_123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d5c69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Soumil! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke([\n",
    "    human_msg\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9c86db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi Soumil! How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 276, 'prompt_tokens': 11, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CzKNf9Z2GoPLKygqxw4hxOOwFpyqq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bd0aa-676c-7570-97f8-cc082e033218-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 11, 'output_tokens': 276, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a09c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 × 99 = 9,801\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "ai_msg = AIMessage(\"I'd be happy to help you with anything\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are an helpful assistant.\"),\n",
    "    HumanMessage(\"Can you help me ?\"),\n",
    "    ai_msg,\n",
    "    HumanMessage(\"Great!, What is 99*99\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82e1a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 47,\n",
       " 'output_tokens': 82,\n",
       " 'total_tokens': 129,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 64}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6227ec8",
   "metadata": {},
   "source": [
    "### Tool Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f0ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c14ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls = [{\n",
    "        \"name\":\"get_weather\",\n",
    "        \"args\":{\"location\":\"New Delhi\"},\n",
    "        \"id\":\"call_123\"\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b22f8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_result = \"Sunny, 30 C\"\n",
    "tool_message = ToolMessage(\n",
    "    content = weather_result,\n",
    "    tool_call_id = \"call_123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe14ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s currently sunny in New Delhi, around 30°C. Would you like the forecast too?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\"What is the weather in New Delhi\"),\n",
    "    ai_message,\n",
    "    tool_message\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
